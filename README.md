# DiffuLex Edge

Edge-optimized diffusion LLM inference framework based on ExecuTorch.

[![Tests](https://img.shields.io/badge/tests-87%20passed-brightgreen)]()
[![Python](https://img.shields.io/badge/python-3.9%2B-blue)]()
[![License](https://img.shields.io/badge/license-MIT-green)]()

## Overview

DiffuLex Edge is a simplified version of the DiffuLex dLLM (diffusion language model) inference framework, designed for deployment on edge devices including iOS, Android, and embedded systems.

### Key Differences from Server Version

| Feature | Server Version | Edge Version |
|---------|---------------|--------------|
| Tensor Parallel | âœ… Multi-GPU | âŒ Single device |
| Attention | Flash Attention (CUDA) | PyTorch SDPA |
| KV Cache | PagedAttention | Static KV Cache |
| Quantization | GPTQ/AWQ/Marlin | XNNPACK/QNN 8-bit |
| Runtime | vLLM | ExecuTorch |

## Project Status

### Implementation Progress

| Phase | Status | Tests |
|-------|--------|-------|
| Phase 1: Model Simplification | âœ… Complete | 34/34 |
| Phase 2: Runtime Implementation | âœ… Complete | 17/17 |
| Phase 3: Quantization | âœ… Complete | 17/17 |
| Phase 4: Multi-Backend Support | âœ… Complete | 10/10 |
| Phase 5: Integration & Testing | ğŸŸ¡ In Progress | Ongoing |

**Total: 87 tests passing, 0 failures**

### Supported Backends

| Backend | Platform | Status |
|---------|----------|--------|
| XNNPACK | ARM64/x86 CPU | âœ… Ready |
| CoreML | Apple Neural Engine | âœ… Ready (macOS/iOS) |
| QNN | Qualcomm NPU | âœ… Ready (Android) |

## Quick Start

### Installation

```bash
# Install dependencies
pip install torch executorch

# Optional: Backend-specific dependencies
pip install coremltools  # For CoreML backend
```

### Basic Usage

```python
from diffulex_edge.model.fast_dllm_v2_edge import FastdLLMV2Edge, FastdLLMV2EdgeConfig
from diffulex_edge.runtime.engine import InferenceEngine, GenerationConfig

# Create model
config = FastdLLMV2EdgeConfig(
    vocab_size=32000,
    hidden_size=512,
    num_hidden_layers=4,
    num_attention_heads=8,
    num_key_value_heads=4,  # GQA
)
model = FastdLLMV2Edge(config)

# Run inference
engine = InferenceEngine.from_model(model)
tokens = engine.generate(
    prompt_tokens=[1, 2, 3],
    config=GenerationConfig(max_new_tokens=20)
)
```

### Model Export

```python
from diffulex_edge.backends import XNNPACKBackend, BackendConfig

# Export with XNNPACK backend
backend = XNNPACKBackend(BackendConfig(
    quantize=True,
    quantization_mode="weight_only"
))

result = backend.export(model, example_inputs)
if result.success:
    with open("model.pte", "wb") as f:
        f.write(result.buffer)
```

## Project Structure

```
diffulex_edge/
â”œâ”€â”€ model/              # Simplified model implementation
â”‚   â”œâ”€â”€ fast_dllm_v2_edge.py   # Edge model with KV cache
â”‚   â””â”€â”€ kv_cache.py            # Static KV cache
â”œâ”€â”€ runtime/            # Inference runtime
â”‚   â”œâ”€â”€ engine.py              # Inference engine
â”‚   â””â”€â”€ sampler.py             # Sampling strategies
â”œâ”€â”€ export/             # Model export
â”‚   â”œâ”€â”€ exporter.py            # ExecuTorch exporter
â”‚   â””â”€â”€ config.py              # Export configuration
â”œâ”€â”€ quant/              # Quantization
â”‚   â”œâ”€â”€ quantizer.py           # PT2E quantizer
â”‚   â””â”€â”€ observers.py           # Quantization observers
â”œâ”€â”€ backends/           # Backend implementations
â”‚   â”œâ”€â”€ base.py                # Backend abstraction
â”‚   â”œâ”€â”€ xnnpack_backend.py     # XNNPACK CPU backend
â”‚   â”œâ”€â”€ qnn_backend.py         # Qualcomm QNN backend
â”‚   â””â”€â”€ coreml_backend.py      # Apple CoreML backend
â””â”€â”€ tests/              # Test suite
    â”œâ”€â”€ test_model_simplified.py
    â”œâ”€â”€ test_kv_cache.py
    â”œâ”€â”€ test_engine.py
    â”œâ”€â”€ test_export.py
    â”œâ”€â”€ test_quantization.py
    â”œâ”€â”€ test_backends.py
    â””â”€â”€ integration/
        â””â”€â”€ test_full_pipeline.py
```

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Application Layer                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                   DiffuLex Edge Runtime                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Tokenizer  â”‚  â”‚   Sampler   â”‚  â”‚   KV Cache Manager  â”‚  â”‚
â”‚  â”‚  (Hugging   â”‚  â”‚ (Greedy/    â”‚  â”‚    (Static)         â”‚  â”‚
â”‚  â”‚   Face)     â”‚  â”‚  Top-k/p)   â”‚  â”‚                     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚              ExecuTorch Runtime (.pte model)                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Transformer Model                      â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚
â”‚  â”‚  â”‚Embeddingâ”‚â†’ â”‚  Layer  â”‚â†’ â”‚  ...    â”‚â†’ â”‚ LM Headâ”‚ â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚(Attn+MLP)â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚
â”‚  â”‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    Backend Execution Engine                  â”‚
â”‚     XNNPACK (CPU)    â”‚    QNN (Qualcomm)   â”‚  CoreML (ANE)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Features

### Model Features
- âœ… Simplified transformer architecture
- âœ… Grouped Query Attention (GQA)
- âœ… Rotary Position Embedding (RoPE)
- âœ… RMSNorm layer normalization
- âœ… SwiGLU activation in FFN
- âœ… Static KV Cache for incremental inference

### Runtime Features
- âœ… PyTorch eager mode inference
- âœ… ExecuTorch runtime support
- âœ… Multiple sampling strategies (Greedy, Top-K, Top-P)
- âœ… Temperature scaling
- âœ… Repetition penalty
- âœ… Stop sequences

### Export Features
- âœ… Multi-backend export (XNNPACK, CoreML, QNN)
- âœ… Dynamic INT8 quantization
- âœ… Static INT8 quantization
- âœ… Weight-only quantization
- âœ… FP16 casting

## Testing

Run the test suite:

```bash
# Run all tests
python -m pytest diffulex_edge/tests/ -v

# Run specific test module
python -m pytest diffulex_edge/tests/test_model_simplified.py -v
python -m pytest diffulex_edge/tests/test_backends.py -v
```

## Examples

See the `examples/` directory for complete usage examples:

- `edge_inference_example.py` - End-to-end inference demo
- `export_model.py` - Command-line export tool

## Roadmap

- [x] Model simplification and basic architecture
- [x] Static KV Cache implementation
- [x] PyTorch SDPA attention
- [x] Inference engine with generation
- [x] XNNPACK backend support
- [x] CoreML backend support
- [x] QNN backend support
- [x] Quantization (dynamic, static, weight-only)
- [ ] End-to-end benchmark suite
- [ ] Mobile deployment examples (iOS/Android)
- [ ] Model compression techniques

## License

MIT License - See LICENSE file for details.

## Acknowledgments

This project is based on the DiffuLex dLLM framework, adapted for edge deployment using Meta's ExecuTorch runtime.
